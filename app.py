# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19tXI-QZMqUJLdMrynC89OFn3r6N5Zy3X
"""

#!pip install Flask onnxruntime numpy

import flask
import onnxruntime as ort
import numpy as np
import os

app = flask.Flask(__name__)

# --- Configuration ---
MODEL_PATH = 'student_model.onnx' # Make sure this file is deployed with your app
INPUT_NAME = None # Will be fetched from the model
OUTPUT_NAME = None # Will be fetched from the model
EXPECTED_INPUT_SHAPE = (1, 6) # batch_size=1, 6 features
# --- End Configuration ---

session = None

def load_model():
    global session, INPUT_NAME, OUTPUT_NAME
    if not os.path.exists(MODEL_PATH):
        print(f"ERROR: Model file not found at {MODEL_PATH}")
        return False
    try:
        print(f"Loading ONNX model from {MODEL_PATH}...")
        # Consider execution providers like 'CPUExecutionProvider' if needed
        session = ort.InferenceSession(MODEL_PATH, providers=['CPUExecutionProvider'])
        INPUT_NAME = session.get_inputs()[0].name
        OUTPUT_NAME = session.get_outputs()[0].name
        print(f"Model loaded successfully.")
        print(f"Input Name: {INPUT_NAME}, Expected Shape: {EXPECTED_INPUT_SHAPE}")
        print(f"Output Name: {OUTPUT_NAME}")
        return True
    except Exception as e:
        print(f"Error loading ONNX model: {e}")
        session = None
        return False

@app.route('/infer', methods=['POST'])
def infer():
    if session is None:
        print("Model not loaded, attempting reload...")
        if not load_model():
             return flask.jsonify({"error": "Model not loaded on server"}), 500

    try:
        data = flask.request.get_json()
        if not data or 'inputs' not in data:
            return flask.jsonify({"error": "Missing 'inputs' key in JSON payload"}), 400

        input_values = data['inputs']

        if not isinstance(input_values, list) or len(input_values) != EXPECTED_INPUT_SHAPE[1]:
             return flask.jsonify({"error": f"Expected a list of {EXPECTED_INPUT_SHAPE[1]} float values in 'inputs'"}), 400

        # Convert input list to numpy array with correct type and shape
        input_array = np.array(input_values, dtype=np.float32).reshape(EXPECTED_INPUT_SHAPE)

        # Run inference
        feeds = {INPUT_NAME: input_array}
        results = session.run([OUTPUT_NAME], feeds) # results is a list of numpy arrays

        # Extract the output data (should be shape [1, 4])
        output_data = results[0].flatten().tolist() # Flatten [1,4] to [4] and convert to Python list

        print(f"Received input: {input_values}, Produced output: {output_data}")
        return flask.jsonify({"outputs": output_data})

    except Exception as e:
        print(f"Error during inference: {e}")
        return flask.jsonify({"error": f"Inference error: {str(e)}"}), 500

@app.route('/')
def home():
    # Simple check to see if the server is running
    status = "Model Loaded" if session else "Model NOT Loaded"
    return f"ONNX Inference Server is running. Status: {status}"

# Load the model when the Flask app starts
if not load_model():
    print("CRITICAL: Failed to load model on startup.")
    # Depending on your deployment, you might want the app to exit or keep running
    # exit(1) # Uncomment to force exit if model fails to load

if __name__ == '__main__':
    # For local testing (Render will use a different command like gunicorn)
    app.run(host='0.0.0.0', port=5000)
